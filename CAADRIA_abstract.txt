Latent Maps for Architectural Reasoning: Multi‑Modal 
Precedent Retrieval as Real‑Time Context for Design 
Alt: Designer-Steerable Fusion for Precedent Search: System and Protocol 
Background / Introduction 
Early-stage architectural design is driven by multi-modal reasoning across form, space, and 
context. Yet most digital precedent search remains text-first, and even recent image-only 
retrieval tools struggle to surface circulation archetypes (e.g., double-loaded corridors, 
courtyard blocks) or to expose why a result is relevant. Black-box ranking undermines 
designer agency, limiting trust and adoption. This work addresses a gap at the intersection of 
artificial intelligence, design cognition and HCI, and interactive environments: how to 
compose heterogeneous evidence (visual appearance, plan-level structure, coarse attributes) 
into a retrieval experience that is both responsive and legible to designers. We propose a 
system that (1) makes the blend of evidence an explicit, reversible, first-class interaction and 
(2) provides patch-aware explanations that highlight local motif correspondences architects 
actually read (e.g., balcony cadence, aperture rhythm). The contribution is a practical, 
extensible template for explainable, human-steerable precedent search. 
Objective / Purpose 
We present a system and evaluation protocol for multi-modal precedent retrieval that centers 
designer-steerable late-fusion and patch-aware explanations. The research questions are: 
RQ1: Does exposing control over the blend of visual, spatial, and attribute evidence increase 
perceived agency and perceived relevance compared to baselines? 
RQ2: Do lightweight plan-derived descriptors measurably improve ranking quality for 
circulation-led queries? 
RQ3: What UI patterns make evidence composition readable without disrupting flow in 
early-stage work? 
Methods 
Representation. Projects are encoded via three streams: (i) global image embeddings of 
photographs/drawings (self-supervised ViT encoder), (ii) plan-derived spatial descriptors 
computed from raster plans (elongation, convexity, corridor ratio, room count), and (iii) project 
attributes using coarse, auditable vocabularies (typology, massing, climate bin; optional WWR 
band). 
Fusion. Per-channel scores are z-score normalized and combined by a designer-set tri-slider 
with weights α, β, γ (α+β+γ=1) into a fused distance: 
D = α·d_visual + β·d_spatial + γ·d_attr. 
Index & Runtime. Approximate nearest neighbor search uses FAISS (IVF+PQ); a lightweight 
backend (FastAPI) returns top-k candidates, which are patch-tiled and re-ranked by minimum 
patch distance to emphasize local motif similarity. 
Interface. An interactive latent map supports exploratory navigation; a side-by-side 
compare tray and patch heatmaps provide “why-similar” rationales; attribute filters (typology, 
massing, climate) and a tri-slider enable explicit trade-offs. 
Data & Schema. A compact corpus (≥300 projects) is organized with a minimal, reproducible 
schema (CSV + per-project JSON) and derived embeddings. The focus is method 
transparency and auditability rather than scale. 
Evaluation Protocol (pre-specified). 
1. Offline benchmark. A curated query–relevance set (≈50 queries, graded relevance 0–3) 
is used to compute nDCG@10 and mAP@10 for three conditions: (a) visual-only, (b) 
visual+attributes, (c) visual+attributes+spatial. 
2. Formative user study (n=12; 6 students, 6 practitioners). Participants complete 8 
retrieval tasks (e.g., “identify three hot-humid perimeter-block precedents with balcony 
rhythm”). Dependent measures: task time, success rate, perceived relevance, 
perceived agency (Likert), and short NASA-TLX. Analysis uses non-parametric tests 
with effect sizes and confidence intervals. All tasks, metrics, and analysis scripts will 
be released for replication. 
Results (system outcomes to be presented) 
The paper reports: (i) a working prototype achieving interactive retrieval for top-k results on a 
compact corpus; (ii) ablation tables comparing visual-only vs. fused channels on the offline 
benchmark; (iii) qualitative evidence from the study on the readability and usefulness of 
patch-aware explanations; and (iv) latency telemetry and engineering trade-offs (IVF settings, 
patch counts). While large-scale generalization is out of scope, the results foreground 
design-relevant similarity and designer control as measurable, reproducible factors, with all 
evaluation assets made public to enable subsequent comparisons. 
Conclusion / Implications 
Treating fusion as an interaction reframes precedent search as explainable evidence 
composition aligned with designer intent, rather than a one-shot query. For CAADRIA 
communities in AI/ML, HCI, interactive environments, and AR/VR/XR, the system 
demonstrates how multi-modal representations can be surfaced in legible, steerable ways 
that support early-stage decision-making. Beyond precedent search, the architecture and 
protocol generalize to context injection in co-design systems (e.g., AR overlays that retrieve 
precedents on demand with “why-similar” badges). The paper contributes a compact schema, 
feature extractors, a baseline fusion recipe, and a reproducible evaluation kit, inviting the field 
to compare—and improve—designer-in-the-loop retrieval. 
Keywords: Architectural Precedent Retrieval, Human–AI Interaction, Multi-Modal Search, 
Explainable AI, Interactive Design Tools 
Core References (5) 
1. Oquab, M., et al. (2023). DINOv2: Learning robust visual features without 
supervision. 
2. Radford, A., et al. (2021). Learning transferable visual models from natural language 
supervision (CLIP). 
3. Johnson, J., Douze, M., & Jégou, H. (2017). Billion-scale similarity search with GPUs 
(FAISS). 
4. Amershi, S., et al. (2019). Guidelines for Human-AI Interaction. CHI. 
5. Dosovitskiy, A., et al. (2020). An Image is Worth 16×16 Words: Transformers for 
Image Recognition at Scale (ViT).